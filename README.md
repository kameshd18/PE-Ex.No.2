# Ex.No: 2 – Evaluation of 2024 Prompting Tools Across Diverse AI Platforms

**Date:** 

**Register Number:** 212222240043  

---

## Aim
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) for a specific use case, such as summarizing text or answering technical questions, by generating prompt-based outputs and analyzing them.

---

## AI Tools Required
- ChatGPT  
- Claude  
- Bard  
- Cohere Command  
- Meta AI Platform  

---

## Explanation / Methodology

### 1. Define the Use Case
- **Task Selected:** Summarizing a technical document into concise, accurate, and clear points.  
- **Reason:** Summarization is applicable across all AI platforms and allows evaluation of accuracy, clarity, depth, and relevance.

### 2. Create a Set of Prompts
- **Prompt 1:** “Summarize the following technical document into 5 key points with clear explanations.”  
- **Prompt 2:** “Provide a brief summary highlighting the most important technical concepts in the text.”  
- **Prompt 3:** “Generate a detailed explanation of the document suitable for a beginner in the field.”  

> All platforms received identical prompts for consistency.

### 3. Run the Experiment on Each AI Platform
- Input the prompts into ChatGPT, Claude, Bard, Cohere Command, and Meta.  
- Record the following:  
  - Response time  
  - Ease of interaction / user interface  
  - Any technical issues  
  - Response content  

### 4. Evaluate Response Quality
Responses were assessed based on:  
- **Accuracy:** Correctness of information  
- **Clarity:** Ease of understanding  
- **Depth:** Level of detail and insight  
- **Relevance:** How closely the response matches the prompt’s intent  

### 5. Compare Performance
- Observed differences across platforms.  
- Identified platform-specific advantages (e.g., speed, clarity, depth).

---

## Comparison Table of AI Platforms

| AI Platform        | Accuracy | Clarity | Depth | Relevance | Notes / Observations |
|------------------|----------|--------|-------|----------|---------------------|
| ChatGPT           | High     | High   | Medium| High     | Clear, accurate, fast response |
| Claude            | High     | Medium | High  | High     | Detailed, slightly slower |
| Bard              | Medium   | High   | Medium| Medium   | Clear but less depth in technical content |
| Cohere Command    | Medium   | Medium | Medium| High     | Good relevance, moderate clarity |
| Meta              | Medium   | Medium | Medium| Medium   | Fast response, lacks depth in complex topics |

---

## Output
The prompts were executed successfully across all AI platforms, and responses were recorded and evaluated.

---

## Conclusion
- ChatGPT and Claude showed the best overall performance for summarization tasks, with ChatGPT providing faster and clearer responses, while Claude offered more detailed explanations.  
- Bard, Cohere Command, and Meta performed adequately but lacked either depth or clarity in complex technical tasks.  
- **Recommendation by use case:**  
  - Quick and clear summaries: **ChatGPT**  
  - In-depth explanations: **Claude**  
  - Speed-focused applications: **Meta**

---

## Result
The prompt for the evaluation of AI platforms executed successfully, and comparative analysis was performed based on accuracy, clarity, depth, and relevance.
