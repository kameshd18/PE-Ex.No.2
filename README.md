# Ex.No: 2 – Evaluation of 2024 Prompting Tools Across Diverse AI Platforms

**Date:** __________  

**Register Number:** 212222240043  

---

## Aim
To evaluate and compare the performance, user experience, and response quality of 2024 AI platforms—**ChatGPT, Claude, Bard, Cohere Command, and Meta**—for a specific use case: *summarizing technical content*. The study focuses on response **accuracy, clarity, depth, and relevance**.

---

## AI Tools Required
- **ChatGPT** (GPT-5 Mini)  
- **Claude** (Anthropic)  
- **Bard** (Google)  
- **Cohere Command** (Cohere AI)  
- **Meta AI Platform** (Meta LLaMA or Meta AI)  

---

## Explanation / Methodology

### 1. Define the Use Case
- **Task Selected:** Summarizing a 1000-word technical document about *Artificial Intelligence applications in healthcare*.  
- **Reason:** Summarization is relevant across all platforms and provides a measurable way to compare **accuracy, clarity, depth, and relevance**.

### 2. Create a Set of Prompts
All prompts are uniform to ensure fair comparison:  

1. **Prompt 1:** “Summarize this document into **5 key points**, highlighting the main AI applications in healthcare.”  
2. **Prompt 2:** “Provide a **concise summary** emphasizing the most critical concepts in the text.”  
3. **Prompt 3:** “Explain the document in **detail suitable for a beginner**, covering examples and practical applications.”  

### 3. Run the Experiment
- Input the prompts into all five platforms under identical conditions:  
  - Same text input  
  - Same prompt wording  
  - Monitored response time  
- Record metrics:  
  - **Response time** (in seconds)  
  - **Ease of interaction** (1–5 scale)  
  - **Technical issues** (if any)  
  - **Response content**  

---

## Evaluation Criteria
Responses were assessed using the following parameters:

| Parameter | Description |
|-----------|-------------|
| Accuracy  | Correctness of technical content |
| Clarity   | Ease of understanding and language simplicity |
| Depth     | Level of detail, examples, and insights |
| Relevance | How closely response matches prompt requirements |

---

## Data & Observations

| AI Platform        | Response Time (s) | Accuracy | Clarity | Depth | Relevance | Notes / Observations |
|------------------|-----------------|---------|--------|-------|----------|---------------------|
| ChatGPT           | 12              | High    | High   | Medium| High     | Clear, fast, concise; suitable for technical summaries |
| Claude            | 15              | High    | Medium | High  | High     | Very detailed; slightly slower response |
| Bard              | 10              | Medium  | High   | Medium| Medium   | Easy to read, lacks depth in technical explanations |
| Cohere Command    | 18              | Medium  | Medium | Medium| High     | Moderate clarity, relevance good, slower for complex tasks |
| Meta              | 8               | Medium  | Medium | Medium| Medium   | Fast response, but less detailed; better for simple summaries |

**Additional Notes:**
- ChatGPT: Best for clarity and speed.  
- Claude: Best for in-depth explanations.  
- Bard: Good for simple summaries and user-friendly output.  
- Cohere Command: Consistent relevance but slower.  
- Meta: Fastest response but limited depth for technical content.  

---

## Output
All prompts executed successfully across all AI platforms. The summarized content was recorded and analyzed according to defined criteria.

---

## Conclusion
- **Best Overall Performance:** ChatGPT and Claude  
- **Strengths by Platform:**  
  - **ChatGPT:** Clear, fast, and accurate summaries  
  - **Claude:** Detailed explanations with examples  
  - **Bard:** Simple, readable summaries  
  - **Cohere Command:** High relevance but slower and less clear  
  - **Meta:** Fast response, limited depth  

- **Recommendation:**  
  - **Quick, clear summaries:** ChatGPT  
  - **In-depth technical explanations:** Claude  
  - **Speed-focused, basic summaries:** Meta  

---

## Result
The prompts for evaluating AI platforms executed successfully. Comparative analysis revealed platform-specific strengths and weaknesses in summarization tasks.  

---

